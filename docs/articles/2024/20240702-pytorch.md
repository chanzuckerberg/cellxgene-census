# First stable iteration of Census (SOMA) PyTorch loaders

*Published:* *July 2nd, 2024*

*By:* *[Pablo Garcia-Nieto](mailto:pgarcia-nieto@chanzuckerberg.com), [Ryan Williams](mailto:pgarcia-nieto@chanzuckerberg.com), [Emanuele Bezzi](mailto:ebezzi@chanzuckerberg.com)*

The Census team is excited to share the release of PyTorch loaders that work out-of-the-box for memory-efficient training across any slice of the >70M cells in Census.

In 2023, we released a beta version of the loaders and we have observed interest from users to utilize them with Census or their own data. For example [Wolf et al.](https://lamin.ai/blog/arrayloader-benchmarks) performed comparisons across different training apporaches and found our loaders to be ideal for _uncached_ training of Census data, albeit with some caveats.

We have continued the development of the loaders in collaboration with our partners at TileDB, and we are happy to announce this release as the first stable iteration. We hope the loaders can accelerate the development of large-scale models of single-cell data by leveraging the following main features:

- **Efficient memory usage with out-of-core training.**
- **Calibrated shuffling of observations (cells).**
- **Out-of-the-box training on all or any slice of Census data.**
- **Cloud-based or local data access.**
- **Custom data encoders.**
- **Incresed training speed.**

Keep on reading for more usage and benchmark details.

## Census PyTorch loaders usage

The loaders are ready to use for PyTorch modelling via the specilized Data Pipe [`ExperimentDataPipe`](https://chanzuckerberg.github.io/cellxgene-census/_autosummary/cellxgene_census.experimental.ml.pytorch.ExperimentDataPipe.html#cellxgene_census.experimental.ml.pytorch.ExperimentDataPipe), which takes advantage of the out-of-core data access TileDB-SOMA offers. 

Please follow our [Training a PyTorch Model](https://chanzuckerberg.github.io/cellxgene-census/notebooks/experimental/pytorch.html) tutorial for a full reproducible example to train a Logistic Regression. 

Shortly, you can intilize `ExperimentDataPipe` to train a model on tongue cells as follows:

\#TODO verify with ebezzi that this code is up to date

```python
import cellxgene_census.experimental.ml as census_ml
import cellxgene_census
import tiledbsoma as soma

experiment = census["census_data"]["homo_sapiens"]

experiment_datapipe = census_ml.ExperimentDataPipe(
    experiment,
    measurement_name="RNA",
    X_name="raw",
    obs_query=soma.AxisQuery(value_filter="tissue_general == 'tongue' and is_primary_data == True"),
    obs_column_names=["cell_type"],
    batch_size=128,
    shuffle=True,
    soma_chunk_size=10_000,
)
```

Then you can perform any PyTorch operations and training.

```python
# Splitting training and test sets
train_datapipe, test_datapipe = experiment_datapipe.random_split(weights={"train": 0.8, "test": 0.2}, seed=1)

# Creating data loader
experiment_dataloader = census_ml.experiment_dataloader(train_datapipe)

# Training a PyTorch model
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model = MODEL().to(device)
model.train()
```

## Census PyTorch loaders main features

### Efficient memory usage with out-of-core training

Thanks to the underlying backend of Census — TileDB-SOMA — the PyTorch loaders take advantage of incremental data materilization of fix size to keep memory usage constant throughout training.

In addition, data is eagerly fetched while training batches are processed so that compute is never idle and waiting for data to be loaded, this feature is particularly useful when fetching Census data directly from the cloud.

Memory usage is defined by the parameters #TODO ask ebezzi to fill out.

### Calibrated shuffling of observations (cells)
